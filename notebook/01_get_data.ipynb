{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preparation Notebook üìä\n",
    "\n",
    "## Introduction üìù\n",
    "\n",
    "Welcome to the \"Data Collection and Preparation\" notebook (01_get_data) üöÄ. In this notebook, we'll focus on the initial steps of data collection, loading, and preparation for your project. We'll be working with various datasets, including Reddit and Twitter data, to create a consolidated and cleaned dataset that will be used for further analysis.\n",
    "\n",
    "## Table of Contents üìë\n",
    "\n",
    "- [Importing Libraries](#importing-libraries)\n",
    "- [Loading Datasets](#loading-datasets)\n",
    "    - [Show dataframe](#show-dataframe)\n",
    "- [Concatenating Datasets](#concatenating-datasets)\n",
    "- [Data Cleaning](#data-cleaning)\n",
    "    - [Final Data Summary](#final-data-summary)\n",
    "- [Saving Cleaned Data](#saving-cleaned-data)\n",
    "- [References](#references)\n",
    "\n",
    "## Importing Libraries üìö\n",
    "\n",
    "Let's start by importing the necessary libraries for our data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets üìä\n",
    "\n",
    "We will load the various datasets required for our analysis, including Reddit, Twitter, and other relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./../data/raw_data/Reddit_Data.csv\")\n",
    "df1 = pd.read_csv(\"./../data/raw_data/Test.csv\")\n",
    "df2 = pd.read_csv(\"./../data/raw_data/Train.csv\")\n",
    "df3 = pd.read_csv(\"./../data/raw_data/Twitter_Data.csv\")\n",
    "df4 = pd.read_csv(\"./../data/raw_data/Valid.csv\")\n",
    "df5 = pd.read_csv(\"./../data/raw_data/dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show dataframe üí°\n",
    "\n",
    "Display all dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           clean_comment  category\n",
      "0       family mormon have never tried explain them t...         1\n",
      "1      buddhism has very much lot compatible with chr...         1\n",
      "2      seriously don say thing first all they won get...        -1\n",
      "3      what you have learned yours and only yours wha...         0\n",
      "4      for your own benefit you may want read living ...         1\n",
      "...                                                  ...       ...\n",
      "37244                                              jesus         0\n",
      "37245  kya bhai pure saal chutiya banaya modi aur jab...         1\n",
      "37246              downvote karna tha par upvote hogaya          0\n",
      "37247                                         haha nice          1\n",
      "37248             facebook itself now working bjp‚Äô cell          0\n",
      "\n",
      "[37249 rows x 2 columns]\n",
      "                                                   text  label\n",
      "0     I always wrote this series off as being a comp...      0\n",
      "1     1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
      "2     This movie was so poorly written and directed ...      0\n",
      "3     The most interesting thing about Miryang (Secr...      1\n",
      "4     when i first read about \"berlin am meer\" i did...      0\n",
      "...                                                 ...    ...\n",
      "4995  This is the kind of picture John Lassiter woul...      1\n",
      "4996  A MUST SEE! I saw WHIPPED at a press screening...      1\n",
      "4997  NBC should be ashamed. I wouldn't allow my chi...      0\n",
      "4998  This movie is a clumsy mishmash of various gho...      0\n",
      "4999  Formula movie about the illegitimate son of a ...      0\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "                                                    text  label\n",
      "0      I grew up (b. 1965) watching and loving the Th...      0\n",
      "1      When I put this movie in my DVD player, and sa...      0\n",
      "2      Why do people who do not know what a particula...      0\n",
      "3      Even though I have great interest in Biblical ...      0\n",
      "4      Im a die hard Dads Army fan and nothing will e...      1\n",
      "...                                                  ...    ...\n",
      "39995  \"Western Union\" is something of a forgotten cl...      1\n",
      "39996  This movie is an incredible piece of work. It ...      1\n",
      "39997  My wife and I watched this movie because we pl...      0\n",
      "39998  When I first watched Flatliners, I was amazed....      1\n",
      "39999  Why would this film be so good, but only gross...      1\n",
      "\n",
      "[40000 rows x 2 columns]\n",
      "                                               clean_text  category\n",
      "0       when modi promised ‚Äúminimum government maximum...      -1.0\n",
      "1       talk all the nonsense and continue all the dra...       0.0\n",
      "2       what did just say vote for modi  welcome bjp t...       1.0\n",
      "3       asking his supporters prefix chowkidar their n...       1.0\n",
      "4       answer who among these the most powerful world...       1.0\n",
      "...                                                   ...       ...\n",
      "162975  why these 456 crores paid neerav modi not reco...      -1.0\n",
      "162976  dear rss terrorist payal gawar what about modi...      -1.0\n",
      "162977  did you cover her interaction forum where she ...       0.0\n",
      "162978  there big project came into india modi dream p...       0.0\n",
      "162979  have you ever listen about like gurukul where ...       1.0\n",
      "\n",
      "[162980 rows x 2 columns]\n",
      "                                                   text  label\n",
      "0     It's been about 14 years since Sharon Stone aw...      0\n",
      "1     someone needed to make a car payment... this i...      0\n",
      "2     The Guidelines state that a comment must conta...      0\n",
      "3     This movie is a muddled mish-mash of clich√©s f...      0\n",
      "4     Before Stan Laurel became the smaller half of ...      0\n",
      "...                                                 ...    ...\n",
      "4995  Man, I loved this movie! This really takes me ...      1\n",
      "4996  Recovery is an incredibly moving piece of work...      1\n",
      "4997  You can take the crook out of the joint, but i...      1\n",
      "4998  FUTZ is the only show preserved from the exper...      1\n",
      "4999  \"The Mother\" tells of a recently widowed mid-6...      1\n",
      "\n",
      "[5000 rows x 2 columns]\n",
      "                                                     Text Language      Label\n",
      "0       @Charlie_Corley @Kristine1G @amyklobuchar @Sty...       en  litigious\n",
      "1       #BadBunny: Como dos gotas de agua: Joven se di...       es   negative\n",
      "2       https://t.co/YJNiO0p1JV Flagstar Bank disclose...       en  litigious\n",
      "3       Rwanda is set to host the headquarters of Unit...       en   positive\n",
      "4       OOPS. I typed her name incorrectly (today‚Äôs br...       en  litigious\n",
      "...                                                   ...      ...        ...\n",
      "937849            @Juice_Lemons in the dark. it‚Äôs so good       en   positive\n",
      "937850  8.SSR &amp; Disha Salian case should be solved...       en   negative\n",
      "937851  *ACCIDENT:  Damage Only* - Raleigh Fire Depart...       en   negative\n",
      "937852  @reblavoie So happy for her! She‚Äôs been incred...       en   positive\n",
      "937853                         I'm lost and I'm found but       en   negative\n",
      "\n",
      "[937854 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df, df1, df2, df3, df4, df5, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating Datasets üìÅ\n",
    "\n",
    "Now, let's concatenate the loaded datasets to create a unified dataset for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df.clean_comment, df1.text, df2.text, df3.clean_text, df4.text, df5.Text]\n",
    "\n",
    "df_result = pd.concat(dfs, axis=0)\n",
    "df_result.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning üßπ\n",
    "\n",
    "We will perform data cleaning tasks such as handling missing values and duplicates to ensure the quality of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>family mormon have never tried explain them t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>buddhism has very much lot compatible with chr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seriously don say thing first all they won get...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what you have learned yours and only yours wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for your own benefit you may want read living ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178833</th>\n",
       "      <td>@Juice_Lemons in the dark. it‚Äôs so good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178834</th>\n",
       "      <td>8.SSR &amp;amp; Disha Salian case should be solved...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178835</th>\n",
       "      <td>*ACCIDENT:  Damage Only* - Raleigh Fire Depart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178836</th>\n",
       "      <td>@reblavoie So happy for her! She‚Äôs been incred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1178837</th>\n",
       "      <td>I'm lost and I'm found but</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1178838 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0         family mormon have never tried explain them t...\n",
       "1        buddhism has very much lot compatible with chr...\n",
       "2        seriously don say thing first all they won get...\n",
       "3        what you have learned yours and only yours wha...\n",
       "4        for your own benefit you may want read living ...\n",
       "...                                                    ...\n",
       "1178833            @Juice_Lemons in the dark. it‚Äôs so good\n",
       "1178834  8.SSR &amp; Disha Salian case should be solved...\n",
       "1178835  *ACCIDENT:  Damage Only* - Raleigh Fire Depart...\n",
       "1178836  @reblavoie So happy for her! She‚Äôs been incred...\n",
       "1178837                         I'm lost and I'm found but\n",
       "\n",
       "[1178838 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df_result.replace('', np.nan).dropna().drop_duplicates().reset_index(drop=True).to_frame(name='text')\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Data Summary üìä\n",
    "\n",
    "Let's check the summary of the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1178838 entries, 0 to 1178837\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   text    1178838 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 9.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Processed Data üíæ\n",
    "\n",
    "Finally, we'll save the cleaned dataset to a compressed CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_parquet('./../data/Text_dataset.br', engine='pyarrow',compression='brotli', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion üìù\n",
    "\n",
    "This notebook demonstrates the initial steps of data collection, loading, and preparation. We've successfully concatenated and cleaned the data from various sources to create a unified and cleaned dataset. This dataset can now serve as a foundation for more advanced analysis and modeling tasks in your project.\n",
    "\n",
    "Happy coding! üéâ\n",
    "\n",
    "## References üìö\n",
    "\n",
    "Sources of data\n",
    "- [Twitter and Reddit Sentimental analysis Dataset](https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset)\n",
    "- [Sentiment Dataset with 1 Million Tweets](https://www.kaggle.com/datasets/tariqsays/sentiment-dataset-with-1-million-tweets)\n",
    "- [IMDB dataset (Sentiment analysis) in CSV format](https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-analysis-in-csv-format)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
